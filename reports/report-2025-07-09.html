<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Research Report - 2025-07-09</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            border-bottom: 3px solid #007bff;
            padding-bottom: 10px;
        }
        .ai-summary {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #007bff;
        }
        .paper {
            border: 1px solid #ddd;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
            background: #fafafa;
        }
        .paper h3 {
            margin-top: 0;
            color: #007bff;
        }
        .paper a {
            color: #007bff;
            text-decoration: none;
        }
        .paper a:hover {
            text-decoration: underline;
        }
        .meta {
            color: #666;
            font-size: 14px;
            margin: 20px 0;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸ¤– AI Research Report</h1>
        <div class="meta">
            <strong>Date:</strong> 2025-07-09<br>
            <strong>Topics:</strong> large language models, artificial general intelligence, AI safety, robotics AI<br>
            <strong>Papers Found:</strong> 10
        </div>
        
        
        <div class="ai-summary">
            <h2>ðŸ¤– AI Summary</h2>
            <p>The recent advancements in AI research span a diverse range of applications, from remote sensing and human motion analysis to robotics and information retrieval, each addressing unique challenges and proposing innovative solutions. In remote sensing, the RSRefSeg 2 introduces a decoupling paradigm that refines image segmentation by integrating foundation models like CLIP and SAM, leading to improved accuracy and semantic interpretation. This highlights a trend towards leveraging pre-trained models for enhanced cross-modal tasks. Similarly, the AnthroTAP framework for point tracking in human motion leverages automated pseudo-labeling using the SMPL model, achieving state-of-the-art results with significantly reduced data and computational requirements. This underscores the growing emphasis on efficient data utilization and model training in AI research.

In the realm of unsupervised learning, SceneDINO presents a novel approach for semantic scene completion without relying on ground-truth annotations, instead utilizing multi-view consistency and self-supervised learning. This approach facilitates robust 3D scene understanding from single images, marking a shift towards minimizing dependency on labeled data. Meanwhile, the Agent KB framework enhances cross-domain problem solving in language agents by enabling experience reuse, achieving notable improvements in task success rates. This reflects a broader trend of developing AI systems capable of learning from shared experiences and adapting to diverse environments.

Robotic manipulation and information retrieval also see significant advancements. EC-Flow introduces an embodiment-centric flow prediction framework that enhances manipulation capabilities in scenarios involving non-rigid and occluded objects, demonstrating substantial improvements over prior methods. This highlights the importance of integrating embodiment knowledge into AI models for complex tasks. In information retrieval, the EÂ²R-FLOPs framework offers a novel metric for evaluating the efficiency of LLM-based rerankers, addressing the computational challenges of deploying large models. Collectively, these works emphasize a shift towards more efficient, adaptable, and less resource-intensive AI systems, paving the way for broader applicability and scalability across various domains.</p>
        </div>
    
        
        <h2>ðŸ“š Research Papers</h2>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06231v1" target="_blank">RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models</a></h3>
                <p><strong>Authors:</strong> Keyan Chen, Chenyang Liu, Bowen Chen, Jiafan Zhang, Zhengxia Zou, Zhenwei Shi</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.CV</p>
                <p><strong>Summary:</strong> Referring Remote Sensing Image Segmentation provides a flexible and fine-grained framework for remote sensing scene analysis via vision-language collaborative interpretation. Current approaches predominantly utilize a three-stage pipeline encompassing dual-modal encoding, cross-modal interaction, and pixel decoding. These methods demonstrate significant limitations in managing complex semantic relationships and achieving precise cross-modal alignment, largely due to their coupled processing mechanism that conflates target localization with boundary delineation. This architectural coupling amplifies error propagation under semantic ambiguity while restricting model generalizability and interpretability. To address these issues, we propose RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow into a collaborative dual-stage framework: coarse localization followed by fine segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with SAM's segmentation generalizability through strategic foundation model collaboration. Specifically, CLIP is employed as the dual-modal encoder to activate target features within its pre-aligned semantic space and generate localization prompts. To mitigate CLIP's misactivation challenges in multi-entity scenarios described by referring texts, a cascaded second-order prompter is devised, which enhances precision through implicit reasoning via decomposition of text embeddings into complementary semantic subspaces. These optimized semantic prompts subsequently direct the SAM to generate pixel-level refined masks, thereby completing the semantic transmission pipeline. Extensive experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2 surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex semantic interpretation. Code is available at: https://github.com/KyanChen/RSRefSeg2.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06233v1" target="_blank">Learning to Track Any Points from Human Motion</a></h3>
                <p><strong>Authors:</strong> InÃ¨s Hyeonsu Kim, Seokju Cho, Jahyeok Koo, Junghyun Park, Jiahui Huang, Joon-Young Lee, Seungryong Kim</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.CV</p>
                <p><strong>Summary:</strong> Human motion, with its inherent complexities, such as non-rigid deformations, articulated movements, clothing distortions, and frequent occlusions caused by limbs or other individuals, provides a rich and challenging source of supervision that is crucial for training robust and generalizable point trackers. Despite the suitability of human motion, acquiring extensive training data for point tracking remains difficult due to laborious manual annotation. Our proposed pipeline, AnthroTAP, addresses this by proposing an automated pipeline to generate pseudo-labeled training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected humans in video frames, project the resulting 3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle occlusions using ray-casting, and filter out unreliable tracks based on optical flow consistency. A point tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other models trained on real videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to 256 GPUs used in recent state-of-the-art.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06230v1" target="_blank">Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion</a></h3>
                <p><strong>Authors:</strong> Aleksandar JevtiÄ‡, Christoph Reich, Felix Wimbauer, Oliver Hahn, Christian Rupprecht, Stefan Roth, Daniel Cremers</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.CV</p>
                <p><strong>Summary:</strong> Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06229v1" target="_blank">Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving</a></h3>
                <p><strong>Authors:</strong> Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                <p><strong>Summary:</strong> As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other's experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06224v1" target="_blank">EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow</a></h3>
                <p><strong>Authors:</strong> Yixiang Chen, Peiyan Li, Yan Huang, Jiabing Yang, Kehan Chen, Liang Wang</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                <p><strong>Summary:</strong> Current language-guided robotic manipulation systems often require low-level action-labeled datasets for imitation learning. While object-centric flow prediction methods mitigate this issue, they remain limited to scenarios involving rigid objects with clear displacement and minimal occlusion. In this work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly learns manipulation from action-unlabeled videos by predicting embodiment-centric flow. Our key insight is that incorporating the embodiment's inherent kinematics significantly enhances generalization to versatile manipulation scenarios, including deformable object handling, occlusions, and non-object-displacement tasks. To connect the EC-Flow with language instructions and object interactions, we further introduce a goal-alignment module by jointly optimizing movement consistency and goal-image prediction. Moreover, translating EC-Flow to executable robot actions only requires a standard robot URDF (Unified Robot Description Format) file to specify kinematic constraints across joints, which makes it easy to use in practice. We validate EC-Flow on both simulation (Meta-World) and real-world tasks, demonstrating its state-of-the-art performance in occluded object handling (62% improvement), deformable object manipulation (45% improvement), and non-object-displacement tasks (80% improvement) than prior state-of-the-art object-centric flow methods. For more information, see our project website at https://ec-flow1.github.io .</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06223v1" target="_blank">Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers</a></h3>
                <p><strong>Authors:</strong> Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao, Yi Fang</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                <p><strong>Summary:</strong> Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06222v1" target="_blank">Deep Learning Optimization of Two-State Pinching Antennas Systems</a></h3>
                <p><strong>Authors:</strong> Odysseas G. Karagiannidis, Victoria E. Galanopoulou, Panagiotis D. Diamantoulakis, Zhiguo Ding, Octavia Dobre</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.LG</p>
                <p><strong>Summary:</strong> The evolution of wireless communication systems requires flexible, energy-efficient, and cost-effective antenna technologies. Pinching antennas (PAs), which can dynamically control electromagnetic wave propagation through binary activation states, have recently emerged as a promising candidate. In this work, we investigate the problem of optimally selecting a subset of fixed-position PAs to activate in a waveguide, when the aim is to maximize the communication rate at a user terminal. Due to the complex interplay between antenna activation, waveguide-induced phase shifts, and power division, this problem is formulated as a combinatorial fractional 0-1 quadratic program. To efficiently solve this challenging problem, we use neural network architectures of varying complexity to learn activation policies directly from data, leveraging spatial features and signal structure. Furthermore, we incorporate user location uncertainty into our training and evaluation pipeline to simulate realistic deployment conditions. Simulation results demonstrate the effectiveness and robustness of the proposed models.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06221v1" target="_blank">Aligned Textual Scoring Rules</a></h3>
                <p><strong>Authors:</strong> Yuxuan Lu, Yifan Wu, Jason Hartline, Michael J. Curry</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.AI, cs.GT</p>
                <p><strong>Summary:</strong> Scoring rules elicit probabilistic predictions from a strategic agent by scoring the prediction against a ground truth state. A scoring rule is proper if, from the agent's perspective, reporting the true belief maximizes the expected score. With the development of language models, Wu and Hartline (2024) proposes a reduction from textual information elicitation to the numerical (i.e. probabilistic) information elicitation problem, which achieves provable properness for textual elicitation. However, not all proper scoring rules are well aligned with human preference over text. Our paper designs the Aligned Scoring rule (ASR) for text by optimizing and minimizing the mean squared error between a proper scoring rule and a reference score (e.g. human score). Our experiments show that our ASR outperforms previous methods in aligning with human preference while maintaining properness.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06219v1" target="_blank">Is Diversity All You Need for Scalable Robotic Manipulation?</a></h3>
                <p><strong>Authors:</strong> Modi Shi, Li Chen, Jin Chen, Yuxiang Lu, Chiming Liu, Guanghui Ren, Ping Luo, Di Huang, Maoqing Yao, Hongyang Li</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cs.RO, cs.AI, cs.LG</p>
                <p><strong>Summary:</strong> Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of "more diverse is better". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively.</p>
            </div>
        
            <div class="paper">
                <h3><a href="http://arxiv.org/abs/2507.06215v1" target="_blank">Harmonic emission as a probe to coherent transitions in the topological superconductors</a></h3>
                <p><strong>Authors:</strong> Nivash R., S. Srinidhi, Jayendra N. Bandyopadhyay, Amol R. Holkundkar</p>
                <p><strong>Published:</strong> 7/8/2025</p>
                <p><strong>Categories:</strong> cond-mat.other, cond-mat.supr-con, quant-ph</p>
                <p><strong>Summary:</strong> We investigate the dynamical behavior of a topological superconducting system, demonstrating that its static configuration undergoes a transition driven by an intrinsic supercurrent. By analyzing the band population, we confirm the quasiparticle nature of the system both in the presence and absence of an external laser field. Under laser driving, we observe an enhancement in static emission forming a plateau-like structure, accompanied by multiple coherent transitions in the population. These transitions exhibit Rabi-like oscillations, attributed to the presence of Majorana bound states (MBS), further reinforcing the quasiparticle character of the model. Our results highlight the efficacy of laser driving as a probe of the system's topological and dynamical stability.</p>
            </div>
        
        
        <div class="meta">
            <p><em>Generated by AI News Agent</em></p>
        </div>
    </div>
</body>
</html>